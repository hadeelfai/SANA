# -*- coding: utf-8 -*-
"""ÙAi_Sans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11hi9ejh1cbHtylLLqPbXYFReRdUZHspT
"""

# ============================================
# HR Chatbot (Employees + Policies via RAG)
# - Hybrid intent (keywords + LLM + WordNet)
# - Employee answers + Policy answers with source
# - Conversation memory + session timeout
# ============================================



import os, re, json, time
import pandas as pd, numpy as np
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import CrossEncoder
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.document import Document
from rank_bm25 import BM25Okapi

# --- WordNet expansion for English synonyms ---
import nltk
nltk.download("wordnet", quiet=True)
nltk.download("omw-1.4", quiet=True)
from nltk.corpus import wordnet as wn


# ==============================
# 0) API key & file paths
# ==============================
load_dotenv() 
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

EMP_PATH    = os.path.join(BASE_DIR, "Fikrah_cleaned.xlsx")
POLICY_PATH = os.path.join(BASE_DIR, "Fikrah P&C policy .xlsx")
FAISS_DIR   = os.path.join(BASE_DIR, "faiss_hr_index")


# Global memory (conversation only â€“ no LAST_EMPLOYEE now)
conversation_history: list[dict] = []   # will be loaded later from JSON

# ==============================
# 1) Load Employees
# ==============================
employees = pd.read_excel(EMP_PATH)
employees.columns = employees.columns.str.strip().str.lower()

# Map common aliases -> unified keys
aliases = {
    "employee_id":   ["employee_number", "emp id", "emp_id"],
    "email":         ["email_address", "email"],
    "name":          ["full_name", "full_name_en", "name"],
    "job_title":     ["job_name", "position_name"],
    "department":    ["dept_name", "organization_name", "section_name"],
    "leave_balance": ["accrual"],
    "location_name": ["location_name"],
    "grade_name":    ["grade_name"],
}
for k, alts in aliases.items():
    for a in alts:
        if a.lower() in employees.columns:
            employees = employees.rename(columns={a.lower(): k})

print("âœ… Loaded Employees:", len(employees))

# ==============================
# 2) Load Policies + Build FAISS
# ==============================
df = pd.read_excel(POLICY_PATH)

def best_text(row):
    """Pick the best available text for policy search."""
    t = str(row.get("Exact_Rule_Summary", "") or "").strip()
    if not t:
        t = str(row.get("Evidence/Notes", "") or "").strip()
    if not t:
        t = " - ".join(
            [
                str(row.get("Policy_Chapter", "") or ""),
                str(row.get("Policy_Title", "") or ""),
            ]
        ).strip(" -")
    return t

df["text"] = df.apply(best_text, axis=1)

docs = []
for _, row in df.iterrows():
    txt = str(row["text"])
    if len(txt) <= 10:
        continue

    docs.append(
        Document(
            page_content="passage: " + txt,
            metadata={
                "title":      str(row.get("Policy_Title", "") or ""),
                "policy_no":  str(row.get("Clause_ID", "") or ""),
                "department": str(row.get("Policy_Chapter", "") or ""),
            },
        )
    )

embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
vs = FAISS.from_documents(docs, embeddings)
vs.save_local(FAISS_DIR)
print("âœ… Policies indexed:", len(docs))

# Optional keyword retriever (not used below but handy)
TOKENS = [d.page_content.lower().split() for d in docs]
bm25 = BM25Okapi(TOKENS)

# Cross-encoder for light reranking
cross_enc = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# ==============================
# 3) Common helpers
# ==============================
def detect_lang(text: str) -> str:
    """Return 'ar' if Arabic letters are present else 'en'."""
    return "ar" if re.search(r"[\u0600-\u06FF]", text or "") else "en"

def llm_call(messages, model: str = "gpt-4o-mini", temperature: float = 0.0) -> str:
    """Wrapper for OpenAI chat completions."""
    r = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
    )
    return r.choices[0].message.content

def make_json_safe(obj):
    """Convert pandas/numpy/Timestamp values into JSON-safe Python types."""
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if obj is None or (isinstance(obj, float) and np.isnan(obj)):
        return None
    if isinstance(obj, (pd.Timestamp,)):
        return obj.date().isoformat()
    if isinstance(obj, (pd.Series,)):
        return {k: make_json_safe(v) for k, v in obj.to_dict().items()}
    if isinstance(obj, (dict,)):
        return {k: make_json_safe(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return [make_json_safe(v) for v in obj]
    return obj

# ==============================
# 4) Employee helpers (simple behavior)
# ==============================
def find_employee_info(q: str):
    """
    Simple behavior:
    - Try to find employee by:
      1) Employee ID (any 3+ digit number)
      2) Email appearing in the text
      3) Name contained in the text (using 'name' column)
    """
    q_low = (q or "").lower()

    # 1) by numeric employee_id (e.g. 1006)
    if "employee_id" in employees.columns:
        m = re.search(r"\b\d{3,}\b", q_low)
        if m:
            emp_id = m.group()
            row = employees[employees["employee_id"].astype(str) == emp_id]
            if len(row) > 0:
                return row.iloc[0].to_dict()

    # 2) by email (contains)
    if "email" in employees.columns:
        for _, r in employees.iterrows():
            em = str(r.get("email", "")).lower()
            if em and em in q_low:
                return r.to_dict()

    # 3) by name (contains)
    if "name" in employees.columns:
        for _, r in employees.iterrows():
            nm = str(r.get("name", "")).lower()
            if nm and nm in q_low:
                return r.to_dict()

    return None

def g(row: dict, key: str, default: str = "-"):
    """Safe getter for employee dict."""
    val = row.get(key, row.get(key.lower()))
    if isinstance(val, (float, np.floating)) and pd.isna(val):
        return default
    return val if val not in [None, "nan", "NaN"] else default

BASIC_FIELDS_ORDER = [
    ("name",          "Ø§Ù„Ø§Ø³Ù…",           "Name"),
    ("employee_id",   "Ø§Ù„Ø±Ù‚Ù… Ø§Ù„ÙˆØ¸ÙŠÙÙŠ",   "Employee ID"),
    ("job_title",     "Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ", "Job title"),
    ("department",    "Ø§Ù„Ù‚Ø³Ù…",           "Department"),
    ("location_name", "Ø§Ù„Ù…Ù‚Ø±",           "Location"),
    ("hire_date",     "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹ÙŠÙŠÙ†",   "Hire date"),
    ("leave_balance", "Ø±ØµÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§Øª",   "Leave balance"),
    ("email",         "Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ",  "Email"),
]

def render_employee_summary(emp: dict, lang: str) -> str:
    """Return compact, human-friendly summary for one employee."""
    lines = []
    if lang == "ar":
        lines.append("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¸Ù:")
        for key, ar_label, _ in BASIC_FIELDS_ORDER:
            if key in emp or key.lower() in emp:
                lines.append(f"- {ar_label}: {g(emp, key)}")
    else:
        lines.append("Employee info:")
        for key, _, en_label in BASIC_FIELDS_ORDER:
            if key in emp or key.lower() in emp:
                lines.append(f"- {en_label}: {g(emp, key)}")
    return "\n".join(lines)

def render_single_field(emp: dict, field: str, lang: str) -> str:
    """Return one-line answer for a specific employee field."""
    name  = g(emp, "name", "")
    empid = g(emp, "employee_id", "")
    value = g(emp, field, "-")

    labels_ar = {
        "email":              "Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ",
        "department":         "Ø§Ù„Ù‚Ø³Ù…",
        "job_title":          "Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ",
        "hire_date":          "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹ÙŠÙŠÙ†",
        "location_name":      "Ø§Ù„Ù…Ù‚Ø±",
        "grade_name":         "Ø§Ù„Ø¯Ø±Ø¬Ø©",
        "leave_balance":      "Ø±ØµÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§Øª",
        "total_compensation": "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ¹ÙˆÙŠØ¶",
    }
    labels_en = {
        "email":              "Email",
        "department":         "Department",
        "job_title":          "Job title",
        "hire_date":          "Hire date",
        "location_name":      "Location",
        "grade_name":         "Grade",
        "leave_balance":      "Leave balance",
        "total_compensation": "Total compensation",
    }

    label = labels_ar.get(field) if lang == "ar" else labels_en.get(field, field)
    if lang == "ar":
        who = f" Ù„Ù„Ù…ÙˆØ¸Ù {empid}" if empid else ""
        who = f"{who} ({name})" if name else who
        return f"{label}{who}: {value}"
    else:
        who = f" for employee {empid}" if empid else ""
        who = f"{who} ({name})" if name else who
        return f"{label}{who}: {value}"

# --- Arabic normalization (light) ---
AR_DIAC = re.compile(r"[\u064B-\u0652]")

def normalize_ar(s: str) -> str:
    """Light Arabic normalization to help keyword matching."""
    s = (s or "").strip().lower()
    s = AR_DIAC.sub("", s)  # remove diacritics
    s = s.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    s = s.replace("Ù‰", "ÙŠ").replace("Ø¤", "Ùˆ").replace("Ø¦", "ÙŠ")
    s = s.replace("Ø©", "Ù‡")  # taa marbouta -> haa
    s = s.replace("Ù€", "")   # tatweel
    if s.startswith("Ø§Ù„"):   # remove starting "Ø§Ù„"
        s = s[2:]
    return s

# ==============================
# 5) Intent detection (field)
# ==============================
FIELD_ALIASES = {
    "email":              ["email", "mail", "Ø§ÙŠÙ…ÙŠÙ„", "Ø§Ù„Ø¨Ø±ÙŠØ¯", "Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ", "Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ"],
    "department":         ["department", "dept", "Ø§Ù„Ù‚Ø³Ù…", "Ø¥Ø¯Ø§Ø±Ø©", "Ø§Ù„Ø§Ø¯Ø§Ø±Ø©"],
    "job_title":          ["job title", "job", "ÙˆØ¸ÙŠÙØ©", "Ø§Ù„Ù…Ø³Ù…Ù‰", "Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ"],
    "hire_date":          ["hire date", "join date", "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹ÙŠÙŠÙ†", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†Ø¶Ù…Ø§Ù…"],
    "location_name":      ["location", "city", "Ù…ÙˆÙ‚Ø¹", "Ù…Ø¯ÙŠÙ†Ø©", "Ø§Ù„Ù…Ù‚Ø±"],
    "grade_name":         ["grade", "Ø¯Ø±Ø¬Ø©", "Ø§Ù„Ù…Ø³ØªÙˆÙ‰"],
    "leave_balance":      ["leave balance", "leave", "Ø±ØµÙŠØ¯ Ø§Ù„Ø§Ø¬Ø§Ø²Ø§Øª", "Ø±ØµÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§Øª", "Ø§Ù„Ø¥Ø¬Ø§Ø²Ø©", "Ø§Ø¬Ø§Ø²Ø©", "accrual"],
    "total_compensation": ["total compensation", "compensation", "salary total", "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ¹ÙˆÙŠØ¶", "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±Ø§ØªØ¨"],
}

def expand_en_syns(words, max_per_word: int = 6):
    """Expand English keys with WordNet synonyms."""
    out = set()
    for w in words:
        if not w:
            continue
        out.add(w.lower())
        for syn in wn.synsets(w):
            for lemma in syn.lemmas():
                term = lemma.name().replace("_", " ").lower()
                if term.isascii() and len(term) <= 30:
                    out.add(term)
    return list(out)[: max(10, min(50, len(out)))]

def expand_field_aliases(base: dict) -> dict:
    """Add English synonyms (WordNet) into FIELD_ALIASES."""
    expanded = {}
    for field, keys in base.items():
        en = [k for k in keys if re.fullmatch(r"[A-Za-z ][A-Za-z ]*", k or "")]
        ar = [k for k in keys if k and not re.fullmatch(r"[A-Za-z ][A-Za-z ]*", k)]
        en_exp = expand_en_syns(en)
        merged = set(keys) | set(en_exp) | set(ar)
        expanded[field] = sorted({m.strip().lower() for m in merged if m and len(m.strip()) > 0})
    return expanded

FIELD_ALIASES = expand_field_aliases(FIELD_ALIASES)
ALLOWED_FIELDS = list(FIELD_ALIASES.keys())

def detect_requested_field(question: str) -> str | None:
    """Fast keyword-based field detection (Arabic + English)."""
    q_en = (question or "").lower().strip()
    q_ar = normalize_ar(question or "")

    for canonical, keys in FIELD_ALIASES.items():
        for k in keys:
            if not k:
                continue
            # English key
            if re.fullmatch(r"[A-Za-z0-9 _-]+", k):
                if k in q_en:
                    return canonical
            else:  # Arabic key
                if normalize_ar(k) in q_ar:
                    return canonical
    return None

def detect_requested_field_llm(question: str) -> str | None:
    """LLM fallback: map question -> one of ALLOWED_FIELDS or 'general'."""
    system_prompt = (
        "You identify which single HR employee field the user requests.\n"
        "Allowed fields: email, department, job_title, hire_date, "
        "location_name, grade_name, leave_balance, total_compensation.\n"
        "If the question is about general employee info, respond with 'general'. "
        "Return ONLY the field name (lowercase) or 'general'."
    )
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question},
        ],
    )
    field = (resp.choices[0].message.content or "").strip().lower()
    if field in ALLOWED_FIELDS:
        return field
    return None  # treat 'general'/unknown as None

def detect_requested_field_hybrid(question: str) -> str | None:
    """First keyword-based, then fallback to LLM if unclear."""
    f = detect_requested_field(question)
    if f:
        return f
    try:
        return detect_requested_field_llm(question)
    except Exception:
        return None

# ==============================
# 6) Conversation history helpers
# ==============================
def build_history_text(max_messages: int = 6) -> str:
    """Build compact text representation of last N messages for context."""
    lines = []
    for m in conversation_history[-max_messages:]:
        role = m.get("role", "user")
        prefix = "User" if role == "user" else "Assistant"
        content = m.get("content", "")
        lines.append(f"{prefix}: {content}")
    return "\n".join(lines)

# ==============================
# 7) Smalltalk / Greetings
# ==============================
WELCOME_AR = (
    "Ø£Ù‡Ù„Ù‹Ø§ Ø¨Ùƒ ğŸ‘‹\n"
    "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©. Ø£Ø¬Ø§ÙˆØ¨ Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ© ÙˆØ±ØµÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§Øª ÙˆØ¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ†.\n"
    "Ø¬Ø±Ù‘Ø¨: Â«Ù…Ø§ Ù‡ÙŠ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§Øª Ø§Ù„Ø³Ù†ÙˆÙŠØ©ØŸÂ» Ø£Ùˆ Â«Ø£Ø¹Ø·Ù†ÙŠ Ø¨Ø±ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¸Ù 1001Â»."
)
WELCOME_EN = (
    "Welcome ğŸ‘‹\n"
    "I'm the HR assistant. I can answer company policies and employee info.\n"
    "Try: â€œWhat is the annual leave policy?â€ or â€œShow the email of employee 1001â€."
)

def detect_smalltalk(q: str) -> str | None:
    """Detect greeting / thanks / who-are-you / help intents."""
    ql = (q or "").strip().lower()

    if any(x in ql for x in ["Ù…Ø±Ø­Ø¨Ø§", "Ù‡Ù„Ø§", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±",
                             "hi", "hello", "hey", "good morning", "good evening"]):
        return "greet"
    if any(x in ql for x in ["Ø´ÙƒØ±Ø§", "Ø«Ø§Ù†ÙƒØ³", "Ù…Ø´ÙƒÙˆØ±", "thanks", "thank you", "thx"]):
        return "thanks"
    if any(x in ql for x in ["Ù…Ù† Ø§Ù†Øª", "ÙˆØ´ ØªØ³ÙˆÙŠ", "ÙˆØ´ Ø¯ÙˆØ±Ùƒ", "who are you", "what can you do"]):
        return "who"
    if any(x in ql for x in ["Ø³Ø§Ø¹Ø¯Ù†ÙŠ", "Ù…Ø³Ø§Ø¹Ø¯Ø©", "ÙƒÙŠÙ Ø£Ø³ØªØ®Ø¯Ù…", "help", "how to use"]):
        return "help"
    return None

def smalltalk_reply(intent: str, lang: str) -> str:
    """Return canned reply for smalltalk."""
    if intent == "greet":
        return WELCOME_AR if lang == "ar" else WELCOME_EN
    if intent == "thanks":
        return "Ø§Ù„Ø¹ÙÙˆ ğŸ™ØŒ ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¨Ø¹Ø¯ØŸ" if lang == "ar" else "You're welcome ğŸ™. How else can I help?"
    if intent in ("who", "help"):
        return (
            "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ HR: Ø£Ø¬Ø§ÙˆØ¨ Ø§Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ£Ø³ØªØ®Ø±Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ† (Ø¨Ø±ÙŠØ¯ØŒ Ù‚Ø³Ù…ØŒ Ø±ØµÙŠØ¯...). "
            "Ø§ÙƒØªØ¨ Ø±Ù‚Ù… Ø§Ù„Ù…ÙˆØ¸Ù Ø£Ùˆ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø£Ùˆ Ø§Ù„Ø§Ø³Ù…. Ù…Ø«Ø§Ù„: Â«Ø§Ø¹Ø·Ù†ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¸Ù 1001Â»."
        ) if lang == "ar" else (
            "I'm the HR assistant: I answer policies and fetch employee data (email, department, leave...). "
            "Provide employee ID/email/name. Example: â€œShow info for employee 1001â€."
        )
    return WELCOME_AR if lang == "ar" else WELCOME_EN

def translate_ar_to_en(text: str) -> str:
    """
    Translate Arabic HR question to English for better policy retrieval.
    If text is not Arabic or translation fails, return original text.
    """
    if detect_lang(text) != "ar":
        return text

    system = (
        "You are a professional translator for HR/legal content. "
        "Translate the following Arabic question into clear English. "
        "Return ONLY the translation."
    )
    try:
        translation = llm_call(
            [
                {"role": "system", "content": system},
                {"role": "user",   "content": text},
            ],
            model="gpt-4o-mini",
            temperature=0.0,
        )
        return translation.strip()
    except Exception:
        return text

# Rule-based mapping for some Arabic policy phrases
def map_arabic_policy_query(q: str) -> str:
    """Map certain Arabic keywords to richer English queries for retrieval."""
    q_norm = normalize_ar(q)

    if "Ø§Ù†ØªØ¯Ø§Ø¨" in q_norm:
        return "business trip delegation assignment travel policy allowance per diem"
    if "Ø¨Ø¯Ù„ Ø³ÙƒÙ†" in q_norm or "Ø¨Ø¯Ù„ Ø§Ù„Ø³ÙƒÙ†" in q_norm:
        return "housing allowance policy monthly housing 25% of basic salary if no in-kind housing"
    if "Ø¨Ø¯Ù„ Ø§Ù†ØªÙ‚Ø§Ù„" in q_norm or "Ø¨Ø¯Ù„ Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª" in q_norm or "Ø¨Ø¯Ù„ Ù†Ù‚Ù„" in q_norm:
        return "transportation allowance policy monthly transportation allowance"

    return q

# ==============================
# 8) Policy answer via RAG
# ==============================
def answer_policy(q: str, top_k: int = 8) -> tuple[str, dict]:
    """
    Retrieve policy text using FAISS + rerank with cross-encoder.
    For Arabic:
      - apply simple mapping for some keywords
      - translate to English
      - combine both Arabic + English for embeddings
    Returns (context_text, top_meta) where top_meta has:
      - policy_no
      - department
      - title
    """
    lang = detect_lang(q)

    q_for_embed = q
    if lang == "ar":
        q_mapped = map_arabic_policy_query(q)
        q_en = translate_ar_to_en(q_mapped)
        q_for_embed = f"{q_mapped} / {q_en}"

    q_e5 = "query: " + q_for_embed
    docs_sim = vs.similarity_search(q_e5, k=top_k)
    if not docs_sim:
        return "", {}

    pairs = [[q_for_embed, d.page_content] for d in docs_sim]
    scores = cross_enc.predict(pairs).tolist()
    ranked = sorted(zip(docs_sim, scores), key=lambda x: x[1], reverse=True)

    top_snips = []
    top_doc = ranked[0][0]

    top_meta = {
        "policy_no":  top_doc.metadata.get("policy_no", "") or "",
        "department": top_doc.metadata.get("department", "") or "",
        "title":      top_doc.metadata.get("title", "") or "",
    }

    for d, _ in ranked[:3]:
        text = d.page_content.replace("passage: ", "", 1)
        top_snips.append(text)

    context = "\n\n---\n\n".join(top_snips)[:3000]
    return context, top_meta

# ==============================
# 9) Decide when to use employee vs policy
# ==============================
def should_route_to_employee(q: str, requested_field: str | None) -> bool:
    """
    Decide if this query should go to the employee route.
    Rules:
      - If a specific employee field is detected -> employee.
      - If query has strong policy words (policy, allowance, leave, visa, termination...)
        and NO strong "info" words -> treat as policy even if employee ID exists.
      - If query has "info" / "email" / "department" etc and no strong policy words -> employee.
    """
    if requested_field:
        return True

    q_low = (q or "").lower()
    q_norm = normalize_ar(q)

    policy_words_en = [
        "policy", "allowance", "visa", "ticket", "termination",
        "resignation", "end of service", "leave"
    ]
    policy_words_ar = [
        "Ø³ÙŠØ§Ø³Ù‡", "Ø³ÙŠØ§Ø³Ø©", "Ø¨Ø¯Ù„", "Ø§Ø¬Ø§Ø²Ù‡", "Ø§Ø¬Ø§Ø²Ø©",
        "ØªØ°ÙƒØ±Ù‡", "Ø§Ù†Ù‡Ø§Ø¡", "Ù†Ù‡Ø§ÙŠÙ‡ Ø§Ù„Ø®Ø¯Ù…Ù‡", "Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ø©", "Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ù‡"
    ]

    info_words_en = [
        "info", "information", "details", "show", "give me",
        "what is", "who is", "email", "department", "job title"
    ]
    info_words_ar = [
        "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "Ø§Ù„Ø¨Ø±ÙŠØ¯", "Ø§Ù„Ù‚Ø³Ù…", "Ø§Ù„Ù…Ø³Ù…Ù‰", "Ø±ØµÙŠØ¯",
        "Ù…Ø§ Ù‡Ùˆ", "Ù…Ø§Ù‡ÙŠ", "Ù…Ø§ Ù‡ÙŠ", "Ù…Ù† Ù‡Ùˆ"
    ]

    has_policy = any(w in q_low for w in policy_words_en) or any(w in q_norm for w in policy_words_ar)
    has_info   = any(w in q_low for w in info_words_en) or any(w in q_norm for w in info_words_ar)

    # If clearly about policy -> do NOT route to employee
    if has_policy and not has_info:
        return False

    # If clearly asking about employee info -> employee
    if has_info:
        return True

    # If query explicitly mentions "employee" / "Ø§Ù„Ù…ÙˆØ¸Ù" without clear policy words -> employee
    if "employee" in q_low or "Ø§Ù„Ù…ÙˆØ¸Ù" in q_norm:
        return not has_policy

    # Default: not confident -> treat as policy (safer for your use-case)
    return False

# ==============================
# 10) Main router
# ==============================
def hr_answer(q: str) -> str:
    """
    Main router:
    - Smalltalk
    - Employee questions (stateless)
    - Policy questions via RAG (with history only for pronouns)
    """
    global conversation_history

    lang = detect_lang(q)

    # ---- Smalltalk first ----
    st = detect_smalltalk(q)
    if st:
        return smalltalk_reply(st, lang)

    # ---- A) Employee route (with gating) ----
    emp = find_employee_info(q)
    requested_field = None
    if emp:
        requested_field = detect_requested_field_hybrid(q)

    if emp and should_route_to_employee(q, requested_field):
        emp = make_json_safe(emp)
        if requested_field and (requested_field in emp or requested_field.lower() in emp):
            return render_single_field(emp, requested_field, lang)
        return render_employee_summary(emp, lang)

    # ---- B) Policy route (RAG) ----
    context, meta = answer_policy(q)
    if not context:
        if lang == "ar":
            return (
                "Ù„Ù… Ø£Ø¬Ø¯ Ø³ÙŠØ§Ø³Ø© Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©. "
                "Ø­Ø§ÙˆÙ„ Ø£Ù† ØªØ¹ÙŠØ¯ ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„Ùƒ Ø£Ùˆ ØªÙˆØ¶Ù‘Ø­ Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø£ÙƒØ«Ø±."
            )
        else:
            return (
                "I couldn't find any policy related to your question in the current records. "
                "Please rephrase or specify the policy."
            )

    history_text = build_history_text()

    if lang == "ar":
        system = (
            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø´Ø±ÙŠØ© ÙŠØ¬ÙŠØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·. "
            "Ø§Ø¹ØªÙ…Ø¯ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø®Ù„ [CONTEXT] Ù„Ù„Ø­Ù‚Ø§Ø¦Ù‚ ÙˆØ§Ù„Ø³ÙŠØ§Ø³Ø§Øª. "
            "ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… [HISTORY] ÙÙ‚Ø· Ù„ÙÙ‡Ù… Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± ÙˆØ³ÙŠØ§Ù‚ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©ØŒ "
            "Ù„ÙƒÙ† Ù„Ø§ ØªØ¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ [CONTEXT]. "
            "ÙƒÙ† Ù…ÙˆØ¬Ø²Ù‹Ø§ ÙˆØ¯Ù‚ÙŠÙ‚Ù‹Ø§."
        )
    else:
        system = (
            "You are an HR assistant. Answer in English only. "
            "Use ONLY the content in [CONTEXT] for facts/policies. "
            "You may use [HISTORY] only to resolve pronouns and understand the dialogue context, "
            "but do NOT invent facts beyond [CONTEXT]. "
            "Be concise and precise."
        )

    user_prompt = (
        f"[HISTORY]\n{history_text}\n\n"
        f"[QUESTION]\n{q}\n\n"
        f"[CONTEXT]\n{context}"
    )

    messages = [
        {"role": "system", "content": system},
        {"role": "user",   "content": user_prompt},
    ]
    base_answer = llm_call(messages)

    # ---- Add policy source (policy_no + department + title) ----
    policy_no = (meta.get("policy_no") or "").strip()
    dept      = (meta.get("department") or "").strip()
    title     = (meta.get("title") or "").strip()

    if lang == "ar":
        src_parts = []
        if policy_no:
            src_parts.append(f"Ø±Ù‚Ù… Ø§Ù„Ø³ÙŠØ§Ø³Ø©: {policy_no}")
        if dept:
            src_parts.append(f"Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©/Ø§Ù„Ù‚Ø³Ù…: {dept}")
        if title:
            src_parts.append(f"Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {title}")
        if src_parts:
            base_answer += "\n\nØ§Ù„Ù…ØµØ¯Ø±:\n- " + "\n- ".join(src_parts)
    else:
        src_parts = []
        if policy_no:
            src_parts.append(f"Policy No: {policy_no}")
        if dept:
            src_parts.append(f"Department: {dept}")
        if title:
            src_parts.append(f"Title: {title}")
        if src_parts:
            base_answer += "\n\nSource:\n- " + "\n- ".join(src_parts)

    return base_answer


# ==============================
# Entry point for backend usage
# ==============================
if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        question = " ".join(sys.argv[1:])
    else:
        question = input("You: ").strip()

    answer = hr_answer(question)
    print(answer)

